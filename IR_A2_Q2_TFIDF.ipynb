{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IR_A2_Q2_TFIDF.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "kodk55uuL6F-"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2dKlTGegySM"
      },
      "source": [
        "# Mounting the drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNrDDLXSgmHq",
        "outputId": "b739874e-8d58-42ca-f289-8c251e9b38b1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wafu6jUCg5HE"
      },
      "source": [
        "# Importing the required libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-aRNAJxcZ2pu",
        "outputId": "db78073c-c325-4200-a106-5d4f45e4a27f"
      },
      "source": [
        "import os\n",
        "import math\n",
        "import ast\n",
        "import glob\n",
        "import nltk\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from nltk.stem.porter import PorterStemmer\n",
        "!pip install autocorrect\n",
        "from autocorrect import Speller\n",
        "from collections import OrderedDict \n",
        "from pathlib import Path\n",
        "import shutil\n",
        "import spacy \n",
        "import re\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "!pip install contractions\n",
        "!pip install spacy\n",
        "!pip install textblob\n",
        "import contractions\n",
        "from textblob import TextBlob, Word\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting autocorrect\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/08/ba/c1a61e1ad64f62b29a53dc55cec451ded52eb6084a5822566cc41d01a578/autocorrect-2.4.0.tar.gz (622kB)\n",
            "\u001b[K     |████████████████████████████████| 624kB 4.5MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: autocorrect\n",
            "  Building wheel for autocorrect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for autocorrect: filename=autocorrect-2.4.0-cp37-none-any.whl size=621775 sha256=b2665def417998e5e306f5f0b2c50c9197b2316737c908671862c07828975069\n",
            "  Stored in directory: /root/.cache/pip/wheels/1d/b0/d4/b941891ad0f8d8847be03583e21e68ed4732d763c71a6c0943\n",
            "Successfully built autocorrect\n",
            "Installing collected packages: autocorrect\n",
            "Successfully installed autocorrect-2.4.0\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "Collecting contractions\n",
            "  Downloading https://files.pythonhosted.org/packages/0a/04/d5e0bb9f2cef5d15616ebf68087a725c5dbdd71bd422bcfb35d709f98ce7/contractions-0.0.48-py2.py3-none-any.whl\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading https://files.pythonhosted.org/packages/d3/fe/021d7d76961b5ceb9f8d022c4138461d83beff36c3938dc424586085e559/textsearch-0.0.21-py2.py3-none-any.whl\n",
            "Collecting pyahocorasick\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7f/c2/eae730037ae1cbbfaa229d27030d1d5e34a1e41114b21447d1202ae9c220/pyahocorasick-1.4.2.tar.gz (321kB)\n",
            "\u001b[K     |████████████████████████████████| 327kB 5.5MB/s \n",
            "\u001b[?25hCollecting anyascii\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/09/c7/61370d9e3c349478e89a5554c1e5d9658e1e3116cc4f2528f568909ebdf1/anyascii-0.1.7-py3-none-any.whl (260kB)\n",
            "\u001b[K     |████████████████████████████████| 266kB 7.9MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
            "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.2-cp37-cp37m-linux_x86_64.whl size=85398 sha256=be4f2f603710a676f6a920554c6db63f3dcd04080ca3d2c210af0216c003f18c\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/03/34/77e3ece0bba8b86bfac88a79f923b36d805cad63caeba38842\n",
            "Successfully built pyahocorasick\n",
            "Installing collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.1.7 contractions-0.0.48 pyahocorasick-1.4.2 textsearch-0.0.21\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (54.2.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (3.8.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.4.1)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.7/dist-packages (0.15.3)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.7/dist-packages (from textblob) (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxUF2c4k_EKI"
      },
      "source": [
        "# TFIDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8o1tqy6Uuct"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nRuDxXrbv1V"
      },
      "source": [
        "documentsDict = {}\n",
        "\n",
        "def cleaning(filepath,filename,flag):\n",
        "  if(flag==1):\n",
        "    f = open(filepath, 'r', errors = 'ignore',encoding='cp1250').read().strip()\n",
        "    clean_S = contractions.fix(f)\n",
        "    clean_S=re.sub(\"([\\-]{1,2})\", \" \",clean_S)\n",
        "    clean_s=re.sub(\"[\\,]\", \" \",clean_S)\n",
        "    clean_S = re.sub(r'[\\\"\\']*',\"\", clean_S)  #junk symbols\n",
        "    clean_S=re.sub(\"([\\.\\!]{2})\", \" \",clean_S)  #removing punctuations\n",
        "    clean_S = re.sub('\\s+', ' ', clean_S)    \n",
        "    clean_S=re.sub(\"[^a-zA-Z\\s\\n]\", \" \",clean_S)  #used to eliminate non-ascii and numeric symbols \n",
        "    clean_S = re.sub(r\"(.)\\1{2}\",\"\",clean_S)  #for elongated words\n",
        "    clean_S = clean_S.lower()\n",
        "    documentsDict[filename]=clean_S\n",
        "\n",
        "  elif(flag==0):\n",
        "    filepath = contractions.fix(filepath)\n",
        "    filepath=re.sub(\"([\\-]{1,2})\", \" \",filepath)\n",
        "    filepath=re.sub(\"[0-9]\", \"\",filepath)\n",
        "    filepath=re.sub(\"[\\,]\", \" \",filepath)\n",
        "    filepath = re.sub(r'[\\\"\\']*',\"\", filepath)\n",
        "    # filepath = re.sub(r\"(.)\\1{2}\",\"\",filepath)  #for elongated words\n",
        "    filepath=re.sub(\"([\\.\\!]{2,})\", \" \",filepath)\n",
        "    filepath = re.sub('\\s+', ' ', filepath)\n",
        "    filepath=re.sub(\"[^a-zA-Z\\s\\n]\", \" \",filepath)\n",
        "    filepath = re.sub(r\"(.)\\1{2}\",\"\",filepath)  #for elongated words\n",
        "    filepath = filepath.lower()\n",
        "    return filepath"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOLTF5vAolhE"
      },
      "source": [
        "def remove_stopword(flag,query):\n",
        "  stopwordsList = stopwords.words('english')\n",
        "  if(flag==1):\n",
        "    for docname in documentsDict:\n",
        "        for word in documentsDict[docname]:\n",
        "            if word in stopwordsList:\n",
        "                documentsDict[docname].remove(word)\n",
        "  elif(flag==0):\n",
        "      q=[]\n",
        "      for word in query:\n",
        "            if word not in stopwordsList:\n",
        "              q.append(word)\n",
        "      query=q.copy()\n",
        "      return query"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhNqRgfwhWLL"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "goEASDK4DeDA"
      },
      "source": [
        "tempdoc={}\n",
        "for docid in range(470):\n",
        "  temp={}\n",
        "  for col in file2:\n",
        "     if docid in file2.iloc[1][col]:\n",
        "       temp[col]=file2.iloc[1][col][docid]\n",
        "  tempdoc[docid] = temp  \n",
        "f = open('tempdoc.txt', 'w')\n",
        "f.write(str(tempdoc))\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjNY5Wx9RM2L"
      },
      "source": [
        "def create_docFreq(file1):\n",
        "  docFreq={}\n",
        "  stopwordsList = stopwords.words('english')\n",
        "  for column in file1.columns:\n",
        "    if column not in stopwordsList:\n",
        "      docFreq[column] = file1.iloc[0][column]\n",
        "  return docFreq\n",
        "doc_frequency = create_docFreq(file1)\n",
        "f= open(\"/content/drive/MyDrive/IR ASSIGNMENT 2/docFrequency.txt\",\"w\")\n",
        "f.write(str(doc_frequency))\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Z9BW4Oq-vMO"
      },
      "source": [
        "def count_dict(file1):#vocab\n",
        "  termFrequency = {}\n",
        "  stopwordsList = stopwords.words('english')\n",
        "  for column in file1.columns:\n",
        "    if column not in stopwordsList:\n",
        "      xa = ast.literal_eval(file1.iloc[1][column])\n",
        "      listoflist = list(xa.values())\n",
        "      counter = 0\n",
        "      for i in range(0,len(listoflist)):\n",
        "        counter =counter+len(listoflist[i])\n",
        "      termFrequency[column] = counter\n",
        "  return termFrequency\n",
        "dict1 = count_dict(file1)\n",
        "f= open(\"/content/drive/MyDrive/IR/vocab.txt\",\"w\")\n",
        "f.write(str(dict1))\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbPw-L30TPE-"
      },
      "source": [
        "def loadStoredFiles():\n",
        "  f = open('/content/drive/MyDrive/IR/documentsFilename.txt', 'r')\n",
        "  documentfilename = f.read()\n",
        "  f.close()\n",
        "  documentfilename = documentfilename.split(\",\")\n",
        "  file1 = pd.read_csv(\"/content/drive/MyDrive/IR/PostingsFinal.csv\" )\n",
        "  file1 = file1.drop(file1.columns[0],axis=1)\n",
        "  file2 = pd.read_csv(\"/content/drive/MyDrive/IR/FrequencyOfWord.csv\" )\n",
        "  file2 = file2.drop(file2.columns[0],axis=1)\n",
        "  for col in file2:\n",
        "     file2.iloc[1][col] = ast.literal_eval(file2.iloc[1][col])\n",
        "  f = open('/content/drive/MyDrive/IR/tempdoc.txt', 'r')\n",
        "  tempdoc = f.read()\n",
        "  f.close()\n",
        "  tempdoc = ast.literal_eval(tempdoc)\n",
        "  f = open('/content/drive/MyDrive/IR/docFrequency.txt', 'r')\n",
        "  docFrequency = f.read()\n",
        "  f.close()\n",
        "  docFrequency = ast.literal_eval(docFrequency)\n",
        "  f = open('/content/drive/MyDrive/IR/vocab.txt', 'r')\n",
        "  dict1 = f.read()\n",
        "  f.close()\n",
        "  dict1 = ast.literal_eval(dict1)\n",
        "  return dict1,tempdoc,docFrequency,file1,file2,documentfilename"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAQw4heBM4mC"
      },
      "source": [
        "def query_count(query_token,dict1):\n",
        "    list1 = dict.fromkeys(query_token, 0)\n",
        "    for word in query_token:\n",
        "      if (word in dict1):\n",
        "         list1[word] += 1\n",
        "    return list1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxkM11R7G4xa"
      },
      "source": [
        "# Compute TF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IifMuAI0d8Gz"
      },
      "source": [
        "def find_binary(dict1,dict2):\n",
        "    tfDict = dict.fromkeys(dict1,0)\n",
        "    for word,count in dict1.items():\n",
        "      if word in dict2:\n",
        "        count=dict2[word]\n",
        "        if count >0 :\n",
        "          tfDict[word] = 1\n",
        "        else:\n",
        "          tfDict[word] = 0\n",
        "    return tfDict   \n",
        "def forBinary(dict1,dict2,tempdoc):\n",
        "  tf1 = find_binary(dict1,dict2)\n",
        "  tf_doc = dict.fromkeys(tempdoc,0)\n",
        "  for word,c in tempdoc.items():\n",
        "    tf_doc[word]=find_binary(dict1,c)\n",
        "  return tf1,tf_doc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dtdXoT4xVKG"
      },
      "source": [
        "def find_rawCount(dict1,dict2):\n",
        "    tfDict = dict.fromkeys(dict1,0)\n",
        "    for word,count in dict1.items():\n",
        "      if word in dict2:\n",
        "        count=dict2[word]\n",
        "        tfDict[word] = count\n",
        "    return tfDict\n",
        "def forRawCount(dict1,dict2,tempdoc):\n",
        "  tf1 = find_rawCount(dict1,dict2)\n",
        "  tf_doc = dict.fromkeys(tempdoc,0)\n",
        "  for word,c in tempdoc.items():\n",
        "    tf_doc[word]=find_rawCount(dict1,c)\n",
        "  return tf1,tf_doc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbXbLh0gnnLd"
      },
      "source": [
        "def findTF(wordDict, dict1):\n",
        "    dict1Count = len(dict1)\n",
        "    tfDict = dict.fromkeys(dict1,0)\n",
        "    for word,count in dict1.items():\n",
        "      if word in wordDict:\n",
        "        count=wordDict[word]\n",
        "        tfDict[word] = count / float(dict1Count)\n",
        "    return tfDict\n",
        "def forTF(dict1,dict2,tempdoc):\n",
        "  tf1 = findTF(dict2, dict1)\n",
        "  tf_doc = dict.fromkeys(tempdoc,0)\n",
        "  for word,c in tempdoc.items():\n",
        "    tf_doc[word]=findTF(c,dict1)\n",
        "  return tf1,tf_doc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F82z5Ddm6z8w"
      },
      "source": [
        "def findlogTF(wordDict, dict1):\n",
        "    tfDict = dict.fromkeys(dict1,0)\n",
        "    for word,count in dict1.items():\n",
        "          tfDict[word] = 1 + np.log(count)\n",
        "    return tfDict\n",
        "def forLogTF(dict1,dict2,tempdoc):\n",
        "  tf1 = findlogTF(dict2, dict1)\n",
        "  tf_doc = dict.fromkeys(tempdoc,0)\n",
        "  for word,c in tempdoc.items():\n",
        "    tf_doc[word]=findlogTF(c,dict1)\n",
        "  return tf1,tf_doc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5yE74hH7GO-"
      },
      "source": [
        "def finddoublenormTF(wordDict, dict1):\n",
        "    max=0\n",
        "    for word,count in dict1.items():\n",
        "      if max<count:\n",
        "        max=count\n",
        "    tfDict = dict.fromkeys(dict1,0)\n",
        "    for word,count in dict1.items():\n",
        "        tfDict[word] = 0.5+((0.5*(count)/max))\n",
        "    return tfDict\n",
        "def forNorm(dict1,dict2,tempdoc):\n",
        "  tf1 = finddoublenormTF(dict2, dict1)\n",
        "  tf_doc = dict.fromkeys(tempdoc,0)\n",
        "  for word,c in tempdoc.items():\n",
        "    tf_doc[word]=finddoublenormTF(c,dict1)\n",
        "  return tf1,tf_doc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCH6K8iRod4T"
      },
      "source": [
        "# Compute IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQ1gn1jKogaj"
      },
      "source": [
        "def findIDF(doc,dict1,doc_frequency):\n",
        "  idfDict = dict.fromkeys(dict1,0)\n",
        "  for word,c in dict1.items():  \n",
        "    if word in doc:\n",
        "      if (word in doc_frequency):\n",
        "          count1 = doc_frequency[word]\n",
        "          # print(count1)\n",
        "      idfDict[word] = np.log(467 / (float(count1)+1))\n",
        "  return idfDict\n",
        "def forIDF(dict1,dict2,tempdoc,docFrequency):\n",
        "  idfs = findIDF(dict2,dict1,docFrequency)\n",
        "  idf_doc = dict.fromkeys(tempdoc,0)\n",
        "  for word,c in tempdoc.items():\n",
        "    idf_doc[word]=findIDF(c,dict1,docFrequency)\n",
        "  return idfs,idf_doc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_ZYTd-8px1F"
      },
      "source": [
        "# Compute TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F43JtY0spMEL"
      },
      "source": [
        "def findTFIDF(dicttf, idfs):\n",
        "  tfidf = {}\n",
        "  for word, i in dicttf.items():\n",
        "      tfidf[word] = i * idfs[word]\n",
        "  return tfidf\n",
        "def forTFIDF(tf1,idfs,tf_doc,idf_doc):\n",
        "  tfidf1 = findTFIDF(tf1, idfs)\n",
        "  df = pd.DataFrame([tfidf1])\n",
        "  tfidfdict = {}\n",
        "  for docid in range(0,467):\n",
        "    temp = {}\n",
        "    for word in tf_doc[0]:\n",
        "      temp[word] = tf_doc[docid][word]*idf_doc[docid][word]\n",
        "    tfidfdict[docid] = temp\n",
        "  return tfidf1,tfidfdict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25bDvL-kaL6q"
      },
      "source": [
        "# Compute Scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k64WsxLVbZG4"
      },
      "source": [
        "def getRank(doc,query):\n",
        "  score=dict.fromkeys(doc,0)\n",
        "  for i in range(467):\n",
        "    sum=0\n",
        "    for c in query:\n",
        "      sum+=doc[i][c]*query[c]\n",
        "    score[i]=sum\n",
        "  rankDoc = sorted(score, key=score.get, reverse=True)\n",
        "  return rankDoc,score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpNmRwABGcz8"
      },
      "source": [
        "dict1,tempdoc,docFrequency,file1,file2,documentfilename = loadStoredFiles()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNxGXer6iw4a"
      },
      "source": [
        "def callingFun(choice,query,dict1,tempdoc,docFrequency,file1,file2,documentfilename):\n",
        "  query = cleaning(query,\" \",0)\n",
        "  query=query.split()\n",
        "  query = remove_stopword(0,query)\n",
        "  dict2 = query_count(query,dict1) \n",
        "  if choice == 'Binary':\n",
        "    tf1,tf_doc = forBinary(dict1,dict2,tempdoc)\n",
        "  elif choice == 'Raw count':\n",
        "    tf1,tf_doc = forRawCount(dict1,dict2,tempdoc)\n",
        "  elif choice == 'Term Frequency':\n",
        "    tf1,tf_doc = forTF(dict1,dict2,tempdoc)\n",
        "  elif choice == 'Log Normalization':\n",
        "    tf1,tf_doc = forLogTF(dict1,dict2,tempdoc)\n",
        "  elif choice == 'Double Normalization':\n",
        "    tf1,tf_doc = forNorm(dict1,dict2,tempdoc)\n",
        "  idfs,idf_doc = forIDF(dict1,dict2,tempdoc,docFrequency)\n",
        "  tfidf1,tfidfdict = forTFIDF(tf1,idfs,tf_doc,idf_doc)\n",
        "  docranked , scores =getRank(tfidfdict,tfidf1)\n",
        "  return documentfilename, docranked , scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mcQ2AzlaPWa"
      },
      "source": [
        "# Give Query and choice of Weighting Scheme and Get scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSyZvPEt5zil",
        "outputId": "04244da4-db6c-474b-bd81-eaf820e6e678"
      },
      "source": [
        "query = \"THE CRAB AND THE HERON\"\n",
        "choice='Double Normalization'\n",
        "print('-------------------------------'+choice+'-------------------------------------')\n",
        "documentfilename,docranked , scores = callingFun(choice,query,dict1,tempdoc,docFrequency,file1,file2,documentfilename)\n",
        "for i in range(0,5):\n",
        "  print(\"Document Name \"+ str(i+1) + \" : \"+ str(documentfilename[docranked[i]]) +\"    with TFIDF Scores :   \"+ str(scores[docranked[i]]) )\n",
        "choice='Log Normalization'\n",
        "print('---------------------------------'+choice+'---------------------------------------')\n",
        "documentfilename,docranked , scores = callingFun(choice,query,dict1,tempdoc,docFrequency,file1,file2,documentfilename)\n",
        "for i in range(0,5):\n",
        "  print(\"Document Name \"+ str(i+1) + \" : \"+ str(documentfilename[docranked[i]]) +\"    with TFIDF Scores :   \"+ str(scores[docranked[i]]) )\n",
        "choice='Term Frequency'\n",
        "print('-------------------------------'+choice+'-------------------------------------')\n",
        "documentfilename,docranked , scores = callingFun(choice,query,dict1,tempdoc,docFrequency,file1,file2,documentfilename)\n",
        "for i in range(0,5):\n",
        "  print(\"Document Name \"+ str(i+1) + \" : \"+ str(documentfilename[docranked[i]]) +\"    with TFIDF Scores :   \"+ str(scores[docranked[i]]) )\n",
        "choice='Raw count'\n",
        "print('-----------------------------------'+choice+'-----------------------------------------')\n",
        "documentfilename,docranked , scores = callingFun(choice,query,dict1,tempdoc,docFrequency,file1,file2,documentfilename)\n",
        "for i in range(0,5):\n",
        "  print(\"Document Name \"+ str(i+1) + \" : \"+ str(documentfilename[docranked[i]]) +\"    with TFIDF Scores :   \"+ str(scores[docranked[i]]) )\n",
        "choice='Binary'\n",
        "print('-------------------------------------'+choice+'-----------------------------------------')\n",
        "documentfilename,docranked , scores = callingFun(choice,query,dict1,tempdoc,docFrequency,file1,file2,documentfilename)\n",
        "for i in range(0,5):\n",
        "  print(\"Document Name \"+ str(i+1) + \" : \"+ str(documentfilename[docranked[i]]) +\"    with TFIDF Scores :   \"+ str(scores[docranked[i]]) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------------------------Double Normalization-------------------------------------\n",
            "Document Name 1 :  'aesop11.txt'    with TFIDF Scores :   10.549035745704728\n",
            "Document Name 2 :  'crabhern.txt'    with TFIDF Scores :   10.549035745704728\n",
            "Document Name 3 :  'aesopa10.txt'    with TFIDF Scores :   4.162853488120446\n",
            "Document Name 4 :  'fgoose.txt'    with TFIDF Scores :   4.162853488120446\n",
            "Document Name 5 :  'long1-3.txt'    with TFIDF Scores :   4.162853488120446\n",
            "-------------------------------Log Normalization-------------------------------------\n",
            "Document Name 1 :  'aesop11.txt'    with TFIDF Scores :   609.6416903418934\n",
            "Document Name 2 :  'crabhern.txt'    with TFIDF Scores :   609.6416903418934\n",
            "Document Name 3 :  'aesopa10.txt'    with TFIDF Scores :   315.463840318558\n",
            "Document Name 4 :  'fgoose.txt'    with TFIDF Scores :   315.463840318558\n",
            "Document Name 5 :  'long1-3.txt'    with TFIDF Scores :   315.463840318558\n",
            "-------------------------------Term Frequency-------------------------------------\n",
            "Document Name 1 :  'crabhern.txt'    with TFIDF Scores :   2.639848823607283e-07\n",
            "Document Name 2 :  'aesop11.txt'    with TFIDF Scores :   7.538614603429366e-08\n",
            "Document Name 3 :  'timem.hac'    with TFIDF Scores :   3.99899247332415e-08\n",
            "Document Name 4 :  'aesopa10.txt'    with TFIDF Scores :   2.9992443549931125e-08\n",
            "Document Name 5 :  'long1-3.txt'    with TFIDF Scores :   2.9992443549931125e-08\n",
            "-------------------------------Raw count-------------------------------------\n",
            "Document Name 1 :  'crabhern.txt'    with TFIDF Scores :   436.72979863047493\n",
            "Document Name 2 :  'aesop11.txt'    with TFIDF Scores :   124.71690076591484\n",
            "Document Name 3 :  'timem.hac'    with TFIDF Scores :   66.15830277785089\n",
            "Document Name 4 :  'aesopa10.txt'    with TFIDF Scores :   49.61872708338817\n",
            "Document Name 5 :  'long1-3.txt'    with TFIDF Scores :   49.61872708338817\n",
            "-------------------------------Binary-------------------------------------\n",
            "Document Name 1 :  'aesop11.txt'    with TFIDF Scores :   42.019022293601225\n",
            "Document Name 2 :  'crabhern.txt'    with TFIDF Scores :   42.019022293601225\n",
            "Document Name 3 :  'aesopa10.txt'    with TFIDF Scores :   16.539575694462723\n",
            "Document Name 4 :  'fgoose.txt'    with TFIDF Scores :   16.539575694462723\n",
            "Document Name 5 :  'long1-3.txt'    with TFIDF Scores :   16.539575694462723\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kodk55uuL6F-"
      },
      "source": [
        "# top 10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ABOXuuEhxwF",
        "outputId": "9d0d0087-14c0-405a-d778-76c4c1eec2fa"
      },
      "source": [
        "for i in range(0,10):\n",
        "  print(\"Document Name \"+ str(i+1) + \" : \"+ str(documentfilename[docranked[i]]) +\"    with TFIDF Scores :   \"+ str(scores[docranked[i]]) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Document Name 1 :  'aesop11.txt'    with TFIDF Scores :   42.019022293601225\n",
            "Document Name 2 :  'crabhern.txt'    with TFIDF Scores :   42.019022293601225\n",
            "Document Name 3 :  'aesopa10.txt'    with TFIDF Scores :   16.539575694462723\n",
            "Document Name 4 :  'fgoose.txt'    with TFIDF Scores :   16.539575694462723\n",
            "Document Name 5 :  'long1-3.txt'    with TFIDF Scores :   16.539575694462723\n",
            "Document Name 6 :  'missing.txt'    with TFIDF Scores :   16.539575694462723\n",
            "Document Name 7 :  'timem.hac'    with TFIDF Scores :   16.539575694462723\n",
            "Document Name 8 : ['13chil.txt'    with TFIDF Scores :   0.0\n",
            "Document Name 9 :  'yukon.txt'    with TFIDF Scores :   0.0\n",
            "Document Name 10 :  '6napolen.txt'    with TFIDF Scores :   0.0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}