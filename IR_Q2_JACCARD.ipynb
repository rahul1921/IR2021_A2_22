{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IR_Q2_JACCARD.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2_Rt6pYPp2W",
        "outputId": "a2209d86-9da9-4d8c-fac0-f429d0df62c3"
      },
      "source": [
        "import os\n",
        "import glob\n",
        "import nltk\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from nltk.stem.porter import PorterStemmer\n",
        "!pip install autocorrect\n",
        "from autocorrect import Speller\n",
        "from collections import OrderedDict \n",
        "from pathlib import Path\n",
        "import shutil\n",
        "import spacy \n",
        "import re\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "!pip install contractions\n",
        "!pip install spacy\n",
        "!pip install textblob\n",
        "import contractions\n",
        "from textblob import TextBlob, Word\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ast"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting autocorrect\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/08/ba/c1a61e1ad64f62b29a53dc55cec451ded52eb6084a5822566cc41d01a578/autocorrect-2.4.0.tar.gz (622kB)\n",
            "\u001b[K     |████████████████████████████████| 624kB 5.5MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: autocorrect\n",
            "  Building wheel for autocorrect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for autocorrect: filename=autocorrect-2.4.0-cp37-none-any.whl size=621775 sha256=b8d1b0e9b9147fde9c58816123adf001bd96f21d3481c98fe026c485cbc42c80\n",
            "  Stored in directory: /root/.cache/pip/wheels/1d/b0/d4/b941891ad0f8d8847be03583e21e68ed4732d763c71a6c0943\n",
            "Successfully built autocorrect\n",
            "Installing collected packages: autocorrect\n",
            "Successfully installed autocorrect-2.4.0\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "Collecting contractions\n",
            "  Downloading https://files.pythonhosted.org/packages/0a/04/d5e0bb9f2cef5d15616ebf68087a725c5dbdd71bd422bcfb35d709f98ce7/contractions-0.0.48-py2.py3-none-any.whl\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading https://files.pythonhosted.org/packages/d3/fe/021d7d76961b5ceb9f8d022c4138461d83beff36c3938dc424586085e559/textsearch-0.0.21-py2.py3-none-any.whl\n",
            "Collecting pyahocorasick\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7f/c2/eae730037ae1cbbfaa229d27030d1d5e34a1e41114b21447d1202ae9c220/pyahocorasick-1.4.2.tar.gz (321kB)\n",
            "\u001b[K     |████████████████████████████████| 327kB 5.3MB/s \n",
            "\u001b[?25hCollecting anyascii\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/09/c7/61370d9e3c349478e89a5554c1e5d9658e1e3116cc4f2528f568909ebdf1/anyascii-0.1.7-py3-none-any.whl (260kB)\n",
            "\u001b[K     |████████████████████████████████| 266kB 11.4MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
            "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.2-cp37-cp37m-linux_x86_64.whl size=85400 sha256=25b3c14614544b389e3c177cc379799d1d95ee15a6f8894c9d940d996a7f99bf\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/03/34/77e3ece0bba8b86bfac88a79f923b36d805cad63caeba38842\n",
            "Successfully built pyahocorasick\n",
            "Installing collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.1.7 contractions-0.0.48 pyahocorasick-1.4.2 textsearch-0.0.21\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (54.2.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (3.10.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.4.1)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.7/dist-packages (0.15.3)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.7/dist-packages (from textblob) (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLCTTrQgPp5I"
      },
      "source": [
        "paths = []\n",
        "filename = []\n",
        "for (dirpath, dirnames, filenames) in os.walk(\"/content/drive/MyDrive/Classroom/CSE508: Information Retrieval/Assignment 2/stories\"):\n",
        "  for i in filenames:\n",
        "    paths.append(str(dirpath)+str(\"/\")+i)\n",
        "    filename.append(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5j2U0GGcJoWu"
      },
      "source": [
        "#Rahul's cell\n",
        "paths = []\n",
        "filename = []\n",
        "# /content/drive/MyDrive/stories\n",
        "# /content/drive/MyDrive/IR ASSIGNMENT 2/stories\n",
        "for (dirpath, dirnames, filenames) in os.walk(\"/content/drive/MyDrive/stories\"):\n",
        "  for i in filenames:\n",
        "    paths.append(str(dirpath)+str(\"/\")+i)\n",
        "    filename.append(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJByazSI7yFb"
      },
      "source": [
        "documentsDict = {}\n",
        "# filenameOfDoc = {}\n",
        "# global counter \n",
        "# counter = 0\n",
        "def cleaning(filepath,filename,flag):\n",
        "  if(flag==1):\n",
        "    f = open(filepath, 'r', errors = 'ignore',encoding='cp1250').read().strip()\n",
        "    clean_S = contractions.fix(f)\n",
        "    clean_S=re.sub(\"([\\-]{1,2})\", \" \",clean_S)\n",
        "    clean_s=re.sub(\"[\\,]\", \" \",clean_S)\n",
        "    clean_S = re.sub(\"[\\.]\",\" \",clean_S)\n",
        "    clean_S = re.sub(r'[\\!@#$%^&\\*()\\_+={}\\:\\;<>\\?/\\|\\-/\"\\']*',\"\", clean_S)  #junk symbols\n",
        "    # clean_S = re.sub(r\"(.)\\1{2}\",\"\",clean_S)  #for elongated words\n",
        "    clean_S=re.sub(\"([\\.\\!]{2})\", \" \",clean_S)  #removing punctuations\n",
        "    clean_S = re.sub('\\s+', ' ', clean_S)    \n",
        "    clean_S=re.sub(\"[^a-zA-Z\\s\\n]\", \" \",clean_S)  #used to eliminate non-ascii and numeric symbols \n",
        "    clean_S = re.sub(r\"(.)\\1{2}\",\"\",clean_S)  #for elongated words\n",
        "    clean_S = clean_S.lower()\n",
        "    documentsDict[filename]=clean_S\n",
        "    # counter = counter+1\n",
        "    # filenameOfDoc[counter] = filename\n",
        "\n",
        "  elif(flag==0):\n",
        "    filepath = contractions.fix(filepath)\n",
        "    filepath=re.sub(\"([\\-]{1,2})\", \" \",filepath)\n",
        "    filepath=re.sub(\"[0-9]\", \"\",filepath)\n",
        "    # filepath=re.sub(\"[\\,]\", \" \",filepath)\n",
        "    filepath = re.sub(r'[\\!@#$%^&\\*()\\_+={}\\:\\;<>\\?/\\|\\-/\"\\']*',\"\", filepath)\n",
        "    # filepath = re.sub(r\"(.)\\1{2}\",\"\",filepath)  #for elongated words\n",
        "    filepath=re.sub(\"([\\.\\!]{2})\", \" \",filepath)\n",
        "    filepath = re.sub('\\s+', ' ', filepath)\n",
        "    filepath=re.sub(\"[^a-zA-Z\\s\\n]\", \" \",filepath)\n",
        "    filepath = re.sub(r\"(.)\\1{2}\",\"\",filepath)  #for elongated words\n",
        "    filepath = filepath.lower()\n",
        "    return filepath"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAjJb8Ng7yIk"
      },
      "source": [
        "for i in range(0,len(filename)):\n",
        "       cleaning(paths[i],filename[i],1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_o5lQlq97yK8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7dd968a8-8b60-40e5-fa1a-90159545507f"
      },
      "source": [
        "len(documentsDict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "467"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsM69uxU7yNI"
      },
      "source": [
        "f = open('cleanedForJaccard.txt', 'w')\n",
        "f.write(str(documentsDict))\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-L3McGTNyEU"
      },
      "source": [
        "f = open('cleanedForJaccard.txt', 'r')\n",
        "documentsDict = f.read()\n",
        "documentsDict = ast.literal_eval(documentsDict)\n",
        "f.close()\n",
        "documentsFilename = list(documentsDict.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bSysH0J7yRI"
      },
      "source": [
        "def gen_token(flag,query):\n",
        "  tokenizer = RegexpTokenizer(r'\\w+')\n",
        "  if(flag==1):\n",
        "    for docname in documentsDict:\n",
        "        documentsDict[docname] = tokenizer.tokenize(str(documentsDict[docname]))\n",
        "\n",
        "  elif(flag==0):\n",
        "    query = tokenizer.tokenize(query)\n",
        "    return query\n",
        "\n",
        "def remove_stopword(flag,query):\n",
        "  stopwordsList = stopwords.words('english')\n",
        "  if(flag==1):\n",
        "    for docname in documentsDict:\n",
        "        for word in documentsDict[docname]:\n",
        "            if word in stopwordsList:\n",
        "                documentsDict[docname].remove(word)\n",
        "  elif(flag==0):\n",
        "      q=[]\n",
        "      for word in query:\n",
        "            if word not in stopwordsList:\n",
        "              q.append(word)\n",
        "      query=q.copy()\n",
        "      return query\n",
        "\n",
        "def normalise_using_textblob(flag,query):\n",
        "  if(flag==1):       \n",
        "    for docname in documentsDict:\n",
        "        dataList = []\n",
        "        for word in documentsDict[docname]:\n",
        "            w = Word(word)\n",
        "            dataList.append(w.lemmatize())\n",
        "        documentsDict[docname] = dataList   \n",
        "\n",
        "  elif(flag==0):\n",
        "    ans = []\n",
        "    for word in query:\n",
        "      w=Word(word)\n",
        "      ans.append(w.lemmatize())\n",
        "    return ans"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjVwM7k4KEuR"
      },
      "source": [
        "def proquery(query):\n",
        "  queryFinal=[]\n",
        "  query = cleaning(query,\" \",0)\n",
        "  queryFinal = gen_token(0,query)\n",
        "  queryFinal = remove_stopword(0,queryFinal)\n",
        "  queryFinal = normalise_using_textblob(0,queryFinal)\n",
        "  #queryFinal = queryFinal.replace(\",\",\" \").split()\n",
        "  return queryFinal"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WxX7Gyu7yTO"
      },
      "source": [
        "gen_token( 1 , \" \" )\n",
        "remove_stopword(1,\"\")\n",
        "normalise_using_textblob(1,\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzoosYMXxhrO"
      },
      "source": [
        "\n",
        "def jaccard_similarity(list1, list2):\n",
        "    s1 = set(list1)\n",
        "    s2 = set(list2)\n",
        "    return float(len(s1.intersection(s2)) / len(s1.union(s2)))\n",
        "\n",
        "document_list = []\n",
        "document_name_list = []\n",
        "def jaccard_on_documents(documentsDict,query):\n",
        "  jaccard_similar_list = []\n",
        "\n",
        "  for i in documentsDict.keys():\n",
        "    document_name_list.append(i)\n",
        "    document_list.append(documentsDict[i])\n",
        "    jaccard_similar_list.append(jaccard_similarity(documentsDict[i],query))\n",
        "\n",
        "  top_jaccard_pos = []\n",
        "\n",
        "  max1 = 0\n",
        "  pos1 = 0\n",
        "\n",
        "  for i in range(5):\n",
        "    max1 = 0\n",
        "    pos1 = 0\n",
        "    for j in range(0,len(jaccard_similar_list)):\n",
        "      flag = 0\n",
        "      for k in range(len(top_jaccard_pos)):\n",
        "        if j==top_jaccard_pos[k]:\n",
        "          flag = 1\n",
        "      if flag == 0:\n",
        "        temp = jaccard_similar_list[j]\n",
        "        if(temp>max1):\n",
        "          max1 = temp\n",
        "          pos1 = j\n",
        "    max1\n",
        "    pos1\n",
        "    top_jaccard_pos.append(pos1)\n",
        "  return top_jaccard_pos\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YG8hijmMOgO",
        "outputId": "03de5ae6-532f-4516-9361-b1ccf82de200"
      },
      "source": [
        "query = \"THE CRAB AND THE HERON\"\n",
        "tokenquery = proquery(query)\n",
        "top_jaccard_pos = jaccard_on_documents(documentsDict,tokenquery)\n",
        "# print(top_jaccard_pos)\n",
        "print(\"Entered Query: \"+ str(tokenquery))\n",
        "print(\"Top 5 Documents Retrieved\")\n",
        "for i in top_jaccard_pos:\n",
        "  # print(i)\n",
        "  # print(document_list[i])\n",
        "  print(document_name_list[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Entered Query: ['crab', 'heron']\n",
            "Top 5 Documents Retrieved\n",
            "crabhern.txt\n",
            "aesopa10.txt\n",
            "long1-3.txt\n",
            "aesop11.txt\n",
            "fgoose.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUum9rewJQUU",
        "outputId": "6ecff725-e726-4861-8978-cb07b7f70866"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}