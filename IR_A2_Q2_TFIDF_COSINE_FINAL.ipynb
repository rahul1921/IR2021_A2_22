{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IR_A2_Q2_TFIDF_COSINE_FINAL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2dKlTGegySM"
      },
      "source": [
        "# Mounting the drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNrDDLXSgmHq",
        "outputId": "f935fce7-ebf6-48c9-94e3-1bffdfed2cab"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wafu6jUCg5HE"
      },
      "source": [
        "# Importing the required libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-aRNAJxcZ2pu",
        "outputId": "062a0bdc-4d6a-45a2-fbf7-3edc4fb50c9f"
      },
      "source": [
        "import os\n",
        "import math\n",
        "import ast\n",
        "import glob\n",
        "import nltk\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from nltk.stem.porter import PorterStemmer\n",
        "!pip install autocorrect\n",
        "from autocorrect import Speller\n",
        "from collections import OrderedDict \n",
        "from pathlib import Path\n",
        "import shutil\n",
        "import spacy \n",
        "import re\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "!pip install contractions\n",
        "!pip install spacy\n",
        "!pip install textblob\n",
        "import contractions\n",
        "from textblob import TextBlob, Word\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting autocorrect\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/08/ba/c1a61e1ad64f62b29a53dc55cec451ded52eb6084a5822566cc41d01a578/autocorrect-2.4.0.tar.gz (622kB)\n",
            "\r\u001b[K     |▌                               | 10kB 12.7MB/s eta 0:00:01\r\u001b[K     |█                               | 20kB 12.2MB/s eta 0:00:01\r\u001b[K     |█▋                              | 30kB 8.4MB/s eta 0:00:01\r\u001b[K     |██                              | 40kB 7.8MB/s eta 0:00:01\r\u001b[K     |██▋                             | 51kB 5.4MB/s eta 0:00:01\r\u001b[K     |███▏                            | 61kB 6.0MB/s eta 0:00:01\r\u001b[K     |███▊                            | 71kB 6.1MB/s eta 0:00:01\r\u001b[K     |████▏                           | 81kB 6.6MB/s eta 0:00:01\r\u001b[K     |████▊                           | 92kB 6.3MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 102kB 6.7MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 112kB 6.7MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 122kB 6.7MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 133kB 6.7MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 143kB 6.7MB/s eta 0:00:01\r\u001b[K     |████████                        | 153kB 6.7MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 163kB 6.7MB/s eta 0:00:01\r\u001b[K     |█████████                       | 174kB 6.7MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 184kB 6.7MB/s eta 0:00:01\r\u001b[K     |██████████                      | 194kB 6.7MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 204kB 6.7MB/s eta 0:00:01\r\u001b[K     |███████████                     | 215kB 6.7MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 225kB 6.7MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 235kB 6.7MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 245kB 6.7MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 256kB 6.7MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 266kB 6.7MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 276kB 6.7MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 286kB 6.7MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 296kB 6.7MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 307kB 6.7MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 317kB 6.7MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 327kB 6.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 337kB 6.7MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 348kB 6.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 358kB 6.7MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 368kB 6.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 378kB 6.7MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 389kB 6.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 399kB 6.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 409kB 6.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 419kB 6.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 430kB 6.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 440kB 6.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 450kB 6.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 460kB 6.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 471kB 6.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 481kB 6.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 491kB 6.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 501kB 6.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 512kB 6.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 522kB 6.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 532kB 6.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 542kB 6.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 552kB 6.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 563kB 6.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 573kB 6.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 583kB 6.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 593kB 6.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 604kB 6.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 614kB 6.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 624kB 6.7MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: autocorrect\n",
            "  Building wheel for autocorrect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for autocorrect: filename=autocorrect-2.4.0-cp37-none-any.whl size=621775 sha256=bf779931b1ab93aeb1943331b1825cb9cd1ead47328a5ed39a33386e6af6138c\n",
            "  Stored in directory: /root/.cache/pip/wheels/1d/b0/d4/b941891ad0f8d8847be03583e21e68ed4732d763c71a6c0943\n",
            "Successfully built autocorrect\n",
            "Installing collected packages: autocorrect\n",
            "Successfully installed autocorrect-2.4.0\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "Collecting contractions\n",
            "  Downloading https://files.pythonhosted.org/packages/0a/04/d5e0bb9f2cef5d15616ebf68087a725c5dbdd71bd422bcfb35d709f98ce7/contractions-0.0.48-py2.py3-none-any.whl\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading https://files.pythonhosted.org/packages/d3/fe/021d7d76961b5ceb9f8d022c4138461d83beff36c3938dc424586085e559/textsearch-0.0.21-py2.py3-none-any.whl\n",
            "Collecting pyahocorasick\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7f/c2/eae730037ae1cbbfaa229d27030d1d5e34a1e41114b21447d1202ae9c220/pyahocorasick-1.4.2.tar.gz (321kB)\n",
            "\u001b[K     |████████████████████████████████| 327kB 6.5MB/s \n",
            "\u001b[?25hCollecting anyascii\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/09/c7/61370d9e3c349478e89a5554c1e5d9658e1e3116cc4f2528f568909ebdf1/anyascii-0.1.7-py3-none-any.whl (260kB)\n",
            "\u001b[K     |████████████████████████████████| 266kB 9.7MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
            "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.2-cp37-cp37m-linux_x86_64.whl size=85393 sha256=f2a8d959a62e952d91679c422a8753a3dc632e9d1edbd8a9821e1e414291c2e0\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/03/34/77e3ece0bba8b86bfac88a79f923b36d805cad63caeba38842\n",
            "Successfully built pyahocorasick\n",
            "Installing collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.1.7 contractions-0.0.48 pyahocorasick-1.4.2 textsearch-0.0.21\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (54.2.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (3.10.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.7.4.3)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.7/dist-packages (0.15.3)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.7/dist-packages (from textblob) (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxUF2c4k_EKI"
      },
      "source": [
        "# TFIDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8o1tqy6Uuct"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nRuDxXrbv1V"
      },
      "source": [
        "documentsDict = {}\n",
        "\n",
        "def cleaning(filepath,filename,flag):\n",
        "  if(flag==1):\n",
        "    f = open(filepath, 'r', errors = 'ignore',encoding='cp1250').read().strip()\n",
        "    clean_S = contractions.fix(f)\n",
        "    clean_S=re.sub(\"([\\-]{1,2})\", \" \",clean_S)\n",
        "    clean_s=re.sub(\"[\\,]\", \" \",clean_S)\n",
        "    clean_S = re.sub(\"[\\.]\",\" \",clean_S)\n",
        "    clean_S = re.sub(r'[\\!@#$%^&\\*()\\_+={}\\:\\;<>\\?/\\|\\-/\"\\']*',\"\", clean_S)  #junk symbols\n",
        "    # clean_S = re.sub(r\"(.)\\1{2}\",\"\",clean_S)  #for elongated words\n",
        "    clean_S=re.sub(\"([\\.\\!]{2})\", \" \",clean_S)  #removing punctuations\n",
        "    clean_S = re.sub('\\s+', ' ', clean_S)    \n",
        "    clean_S=re.sub(\"[^a-zA-Z\\s\\n]\", \" \",clean_S)  #used to eliminate non-ascii and numeric symbols \n",
        "    clean_S = re.sub(r\"(.)\\1{2}\",\"\",clean_S)  #for elongated words\n",
        "    clean_S = clean_S.lower()\n",
        "    documentsDict[filename]=clean_S\n",
        "    # counter = counter+1\n",
        "    # filenameOfDoc[counter] = filename\n",
        "\n",
        "  elif(flag==0):\n",
        "    filepath = contractions.fix(filepath)\n",
        "    filepath=re.sub(\"([\\-]{1,2})\", \" \",filepath)\n",
        "    filepath=re.sub(\"[0-9]\", \"\",filepath)\n",
        "    # filepath=re.sub(\"[\\,]\", \" \",filepath)\n",
        "    filepath = re.sub(r'[\\!@#$%^&\\*()\\_+={}\\:\\;<>\\?/\\|\\-/\"\\']*',\"\", filepath)\n",
        "    # filepath = re.sub(r\"(.)\\1{2}\",\"\",filepath)  #for elongated words\n",
        "    filepath=re.sub(\"([\\.\\!]{2})\", \" \",filepath)\n",
        "    filepath = re.sub('\\s+', ' ', filepath)\n",
        "    filepath=re.sub(\"[^a-zA-Z\\s\\n]\", \" \",filepath)\n",
        "    filepath = re.sub(r\"(.)\\1{2}\",\"\",filepath)  #for elongated words\n",
        "    filepath = filepath.lower()\n",
        "    return filepath"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOLTF5vAolhE"
      },
      "source": [
        "def remove_stopword(flag,query):\n",
        "  stopwordsList = stopwords.words('english')\n",
        "  if(flag==1):\n",
        "    for docname in documentsDict:\n",
        "        for word in documentsDict[docname]:\n",
        "            if word in stopwordsList:\n",
        "                documentsDict[docname].remove(word)\n",
        "  elif(flag==0):\n",
        "      q=[]\n",
        "      for word in query:\n",
        "            if word not in stopwordsList:\n",
        "              q.append(word)\n",
        "      query=q.copy()\n",
        "      return query"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhNqRgfwhWLL"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1SWx9MvvnGR"
      },
      "source": [
        "file2 = pd.read_csv(\"/content/drive/MyDrive/IR/FrequencyOfWord2.csv\")\n",
        "file2 = file2.drop(file2.columns[0],axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yElYyUXrxNDE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "265c2844-eca0-4035-e71f-cda5c6752405"
      },
      "source": [
        "file2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>bordeaux</th>\n",
              "      <th>mademarchj</th>\n",
              "      <th>unlockmarchto</th>\n",
              "      <th>intertwine</th>\n",
              "      <th>elvish</th>\n",
              "      <th>enchant</th>\n",
              "      <th>songmarcha</th>\n",
              "      <th>fleecy</th>\n",
              "      <th>rokraven</th>\n",
              "      <th>staeorra</th>\n",
              "      <th>marsupial</th>\n",
              "      <th>amex</th>\n",
              "      <th>gecko</th>\n",
              "      <th>tissuealice</th>\n",
              "      <th>conjointly</th>\n",
              "      <th>minaret</th>\n",
              "      <th>oriel</th>\n",
              "      <th>saracenic</th>\n",
              "      <th>upspringing</th>\n",
              "      <th>streamlet</th>\n",
              "      <th>intertangled</th>\n",
              "      <th>tuberose</th>\n",
              "      <th>poppy</th>\n",
              "      <th>bosky</th>\n",
              "      <th>laved</th>\n",
              "      <th>begirt</th>\n",
              "      <th>amphitheatre</th>\n",
              "      <th>musically</th>\n",
              "      <th>diverging</th>\n",
              "      <th>wreathe</th>\n",
              "      <th>effulgence</th>\n",
              "      <th>luxuriance</th>\n",
              "      <th>clematis</th>\n",
              "      <th>eglantine</th>\n",
              "      <th>honeysuckle</th>\n",
              "      <th>overspread</th>\n",
              "      <th>cleanness</th>\n",
              "      <th>luxuriantly</th>\n",
              "      <th>divinest</th>\n",
              "      <th>bidden</th>\n",
              "      <th>...</th>\n",
              "      <th>time</th>\n",
              "      <th>long</th>\n",
              "      <th>seen</th>\n",
              "      <th>well</th>\n",
              "      <th>face</th>\n",
              "      <th>frown</th>\n",
              "      <th>slight</th>\n",
              "      <th>replied</th>\n",
              "      <th>yard</th>\n",
              "      <th>across</th>\n",
              "      <th>walked</th>\n",
              "      <th>cried</th>\n",
              "      <th>morning</th>\n",
              "      <th>good</th>\n",
              "      <th>exclaimed</th>\n",
              "      <th>paw</th>\n",
              "      <th>eye</th>\n",
              "      <th>shading</th>\n",
              "      <th>end</th>\n",
              "      <th>walking</th>\n",
              "      <th>himself</th>\n",
              "      <th>said</th>\n",
              "      <th>road</th>\n",
              "      <th>coming</th>\n",
              "      <th>like</th>\n",
              "      <th>look</th>\n",
              "      <th>looking</th>\n",
              "      <th>carrot</th>\n",
              "      <th>big</th>\n",
              "      <th>great</th>\n",
              "      <th>eating</th>\n",
              "      <th>rocking</th>\n",
              "      <th>porch</th>\n",
              "      <th>front</th>\n",
              "      <th>sat</th>\n",
              "      <th>rabbit</th>\n",
              "      <th>mr</th>\n",
              "      <th>fox</th>\n",
              "      <th>sly</th>\n",
              "      <th>child</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>407</td>\n",
              "      <td>302</td>\n",
              "      <td>230</td>\n",
              "      <td>333</td>\n",
              "      <td>276</td>\n",
              "      <td>28</td>\n",
              "      <td>70</td>\n",
              "      <td>145</td>\n",
              "      <td>94</td>\n",
              "      <td>196</td>\n",
              "      <td>194</td>\n",
              "      <td>115</td>\n",
              "      <td>181</td>\n",
              "      <td>297</td>\n",
              "      <td>83</td>\n",
              "      <td>35</td>\n",
              "      <td>298</td>\n",
              "      <td>5</td>\n",
              "      <td>251</td>\n",
              "      <td>109</td>\n",
              "      <td>132</td>\n",
              "      <td>348</td>\n",
              "      <td>121</td>\n",
              "      <td>175</td>\n",
              "      <td>382</td>\n",
              "      <td>302</td>\n",
              "      <td>232</td>\n",
              "      <td>5</td>\n",
              "      <td>214</td>\n",
              "      <td>238</td>\n",
              "      <td>76</td>\n",
              "      <td>11</td>\n",
              "      <td>25</td>\n",
              "      <td>203</td>\n",
              "      <td>197</td>\n",
              "      <td>27</td>\n",
              "      <td>119</td>\n",
              "      <td>24</td>\n",
              "      <td>15</td>\n",
              "      <td>153</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>{466: 1}</td>\n",
              "      <td>{466: 1}</td>\n",
              "      <td>{466: 1}</td>\n",
              "      <td>{466: 1}</td>\n",
              "      <td>{466: 1}</td>\n",
              "      <td>{466: 1}</td>\n",
              "      <td>{466: 1}</td>\n",
              "      <td>{466: 1}</td>\n",
              "      <td>{466: 1}</td>\n",
              "      <td>{466: 1}</td>\n",
              "      <td>{465: 1}</td>\n",
              "      <td>{465: 1}</td>\n",
              "      <td>{465: 1}</td>\n",
              "      <td>{464: 1}</td>\n",
              "      <td>{463: 1}</td>\n",
              "      <td>{463: 1}</td>\n",
              "      <td>{463: 1}</td>\n",
              "      <td>{463: 1}</td>\n",
              "      <td>{463: 1}</td>\n",
              "      <td>{463: 1}</td>\n",
              "      <td>{463: 1}</td>\n",
              "      <td>{463: 1}</td>\n",
              "      <td>{463: 1}</td>\n",
              "      <td>{463: 1}</td>\n",
              "      <td>{463: 1}</td>\n",
              "      <td>{463: 1}</td>\n",
              "      <td>{463: 1}</td>\n",
              "      <td>{463: 1}</td>\n",
              "      <td>{463: 1}</td>\n",
              "      <td>{463: 1}</td>\n",
              "      <td>{463: 1}</td>\n",
              "      <td>{463: 1}</td>\n",
              "      <td>{463: 1}</td>\n",
              "      <td>{463: 1}</td>\n",
              "      <td>{463: 1}</td>\n",
              "      <td>{463: 1}</td>\n",
              "      <td>{463: 1}</td>\n",
              "      <td>{463: 1}</td>\n",
              "      <td>{463: 1}</td>\n",
              "      <td>{463: 1}</td>\n",
              "      <td>...</td>\n",
              "      <td>{0: 4, 1: 16, 2: 11, 3: 6, 4: 7, 5: 3, 6: 5, 7...</td>\n",
              "      <td>{0: 4, 1: 20, 2: 11, 3: 5, 4: 2, 11: 4, 12: 3,...</td>\n",
              "      <td>{0: 1, 1: 6, 2: 7, 3: 4, 4: 1, 6: 1, 11: 1, 13...</td>\n",
              "      <td>{0: 3, 1: 14, 2: 38, 3: 39, 5: 2, 6: 5, 8: 1, ...</td>\n",
              "      <td>{0: 2, 1: 16, 2: 12, 3: 8, 5: 2, 6: 1, 8: 1, 1...</td>\n",
              "      <td>{0: 1, 1: 1, 6: 1, 20: 7, 37: 1, 49: 1, 58: 3,...</td>\n",
              "      <td>{0: 2, 3: 1, 14: 1, 15: 1, 19: 1, 20: 28, 23: ...</td>\n",
              "      <td>{0: 4, 4: 1, 6: 2, 8: 1, 9: 2, 10: 1, 11: 6, 1...</td>\n",
              "      <td>{0: 1, 2: 6, 3: 1, 4: 1, 14: 1, 20: 1, 22: 1, ...</td>\n",
              "      <td>{0: 4, 1: 1, 2: 1, 3: 1, 12: 1, 13: 1, 14: 3, ...</td>\n",
              "      <td>{0: 1, 1: 1, 2: 1, 3: 2, 13: 2, 14: 4, 15: 1, ...</td>\n",
              "      <td>{0: 5, 2: 5, 3: 2, 6: 1, 11: 2, 14: 4, 15: 3, ...</td>\n",
              "      <td>{0: 4, 1: 2, 2: 5, 3: 6, 5: 2, 6: 1, 8: 1, 13:...</td>\n",
              "      <td>{0: 6, 1: 16, 2: 11, 3: 13, 4: 1, 5: 2, 9: 1, ...</td>\n",
              "      <td>{0: 2, 6: 2, 8: 2, 11: 2, 17: 2, 18: 1, 19: 2,...</td>\n",
              "      <td>{0: 2, 1: 1, 16: 5, 21: 1, 23: 2, 31: 1, 38: 1...</td>\n",
              "      <td>{0: 3, 1: 29, 2: 6, 3: 4, 5: 1, 6: 3, 9: 2, 10...</td>\n",
              "      <td>{0: 1, 66: 1, 174: 1, 361: 1, 422: 1}</td>\n",
              "      <td>{0: 2, 1: 10, 2: 3, 3: 5, 5: 2, 8: 2, 11: 1, 1...</td>\n",
              "      <td>{0: 2, 13: 1, 14: 2, 18: 1, 20: 12, 21: 3, 34:...</td>\n",
              "      <td>{0: 2, 4: 1, 6: 3, 8: 1, 11: 1, 16: 1, 17: 1, ...</td>\n",
              "      <td>{0: 5, 1: 8, 2: 33, 3: 43, 4: 7, 6: 5, 8: 3, 9...</td>\n",
              "      <td>{0: 2, 1: 9, 2: 11, 5: 9, 12: 3, 13: 1, 14: 3,...</td>\n",
              "      <td>{0: 1, 1: 6, 2: 1, 3: 1, 14: 1, 18: 2, 20: 41,...</td>\n",
              "      <td>{0: 2, 1: 53, 2: 6, 3: 10, 4: 5, 5: 2, 6: 1, 8...</td>\n",
              "      <td>{0: 2, 1: 10, 2: 3, 3: 7, 4: 1, 5: 1, 8: 2, 14...</td>\n",
              "      <td>{0: 1, 1: 1, 2: 2, 3: 2, 12: 2, 14: 5, 15: 2, ...</td>\n",
              "      <td>{0: 1, 66: 1, 74: 1, 86: 3, 344: 2}</td>\n",
              "      <td>{0: 2, 1: 13, 2: 1, 3: 3, 4: 1, 5: 1, 6: 2, 8:...</td>\n",
              "      <td>{0: 2, 1: 6, 2: 7, 3: 5, 4: 1, 5: 3, 6: 2, 7: ...</td>\n",
              "      <td>{0: 1, 1: 1, 5: 1, 13: 1, 16: 1, 21: 3, 26: 1,...</td>\n",
              "      <td>{0: 1, 20: 5, 58: 1, 74: 2, 144: 3, 195: 2, 22...</td>\n",
              "      <td>{0: 3, 21: 2, 45: 1, 74: 3, 96: 2, 102: 1, 128...</td>\n",
              "      <td>{0: 1, 1: 2, 2: 5, 3: 1, 6: 1, 14: 4, 18: 1, 2...</td>\n",
              "      <td>{0: 2, 1: 5, 2: 5, 6: 1, 8: 1, 13: 1, 14: 5, 1...</td>\n",
              "      <td>{0: 33, 5: 1, 18: 1, 54: 1, 58: 1, 60: 1, 65: ...</td>\n",
              "      <td>{0: 27, 2: 39, 3: 32, 10: 2, 14: 56, 15: 37, 1...</td>\n",
              "      <td>{0: 30, 28: 1, 37: 2, 45: 122, 48: 67, 60: 1, ...</td>\n",
              "      <td>{0: 26, 15: 1, 27: 1, 37: 1, 38: 2, 45: 1, 48:...</td>\n",
              "      <td>{0: 1, 1: 9, 5: 1, 6: 3, 11: 2, 14: 4, 16: 3, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2 rows × 47672 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   bordeaux  ...                                              child\n",
              "0         1  ...                                                153\n",
              "1  {466: 1}  ...  {0: 1, 1: 9, 5: 1, 6: 3, 11: 2, 14: 4, 16: 3, ...\n",
              "\n",
              "[2 rows x 47672 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "un41J4obxmCo"
      },
      "source": [
        "for column in file2.columns:\n",
        "  xa = ast.literal_eval(file2.iloc[1][column])\n",
        "  file2.iloc[1][column] = xa"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "goEASDK4DeDA"
      },
      "source": [
        "tempdoc={}\n",
        "for docid in range(467):\n",
        "  temp={}\n",
        "  for col in file2:\n",
        "     if docid in file2.iloc[1][col]:\n",
        "       temp[col]=file2.iloc[1][col][docid]\n",
        "  tempdoc[docid] = temp  \n",
        "f = open('/content/drive/MyDrive/IR/tempdoc.txt', 'w')\n",
        "f.write(str(tempdoc))\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37kWSMrwAWAF"
      },
      "source": [
        "file1 = pd.read_csv(\"/content/drive/MyDrive/IR/PostingsLemFinal.csv\" )\n",
        "file1 = file1.drop(file1.columns[0],axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QvfV2FdAe91"
      },
      "source": [
        "for col in file1:\n",
        "     file1.iloc[1][col] = ast.literal_eval(file1.iloc[1][col])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjNY5Wx9RM2L"
      },
      "source": [
        "def create_docFreq(file1):\n",
        "  docFreq={}\n",
        "  stopwordsList = stopwords.words('english')\n",
        "  for column in file1.columns:\n",
        "    if column not in stopwordsList:\n",
        "      docFreq[column] = file1.iloc[0][column]\n",
        "  return docFreq\n",
        "doc_frequency = create_docFreq(file1)\n",
        "f= open(\"/content/drive/MyDrive/IR/docFrequency.txt\",\"w\")\n",
        "f.write(str(doc_frequency))\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Z9BW4Oq-vMO"
      },
      "source": [
        "def count_dict(file1):#vocab\n",
        "  termFrequency = {}\n",
        "  stopwordsList = stopwords.words('english')\n",
        "  for column in file1.columns:\n",
        "    if column not in stopwordsList:\n",
        "      xa = ast.literal_eval(file1.iloc[1][column])\n",
        "      listoflist = list(xa.values())\n",
        "      counter = 0\n",
        "      for i in range(0,len(listoflist)):\n",
        "        counter =counter+len(listoflist[i])\n",
        "      termFrequency[column] = counter\n",
        "  return termFrequency\n",
        "dict1 = count_dict(file1)\n",
        "f= open(\"/content/drive/MyDrive/IR/vocab.txt\",\"w\")\n",
        "f.write(str(dict1))\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbPw-L30TPE-"
      },
      "source": [
        "def loadStoredFiles():\n",
        "  f = open('/content/drive/MyDrive/IR/documentsFilenameLemFinal.txt', 'r')\n",
        "  documentfilename = f.read()\n",
        "  f.close()\n",
        "  documentfilename = documentfilename.split(\",\")\n",
        "  file1 = pd.read_csv(\"/content/drive/MyDrive/IR/PostingsLemFinal.csv\" )\n",
        "  file1 = file1.drop(file1.columns[0],axis=1)\n",
        "  file2 = pd.read_csv(\"/content/drive/MyDrive/IR/FrequencyOfWord2.csv\" )\n",
        "  file2 = file2.drop(file2.columns[0],axis=1)\n",
        "  for col in file2:\n",
        "     file2.iloc[1][col] = ast.literal_eval(file2.iloc[1][col])\n",
        "  f = open('/content/drive/MyDrive/IR/tempdoc.txt', 'r')\n",
        "  tempdoc = f.read()\n",
        "  f.close()\n",
        "  tempdoc = ast.literal_eval(tempdoc)\n",
        "  f = open('/content/drive/MyDrive/IR/docFrequency.txt', 'r')\n",
        "  docFrequency = f.read()\n",
        "  f.close()\n",
        "  docFrequency = ast.literal_eval(docFrequency)\n",
        "  f = open('/content/drive/MyDrive/IR/vocab.txt', 'r')\n",
        "  dict1 = f.read()\n",
        "  f.close()\n",
        "  dict1 = ast.literal_eval(dict1)\n",
        "  return dict1,tempdoc,docFrequency,file1,file2,documentfilename"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAQw4heBM4mC"
      },
      "source": [
        "def query_count(query_token,dict1):\n",
        "    list1 = dict.fromkeys(query_token, 0)\n",
        "    for word in query_token:\n",
        "      if (word in dict1):\n",
        "         list1[word] += 1\n",
        "    return list1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxkM11R7G4xa"
      },
      "source": [
        "# Compute TF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IifMuAI0d8Gz"
      },
      "source": [
        "def find_binary(dict1,dict2):\n",
        "    tfDict = dict.fromkeys(dict1,0)\n",
        "    for word,count in dict1.items():\n",
        "      if word in dict2:\n",
        "        count=dict2[word]\n",
        "        if count >0 :\n",
        "          tfDict[word] = 1\n",
        "        else:\n",
        "          tfDict[word] = 0\n",
        "    return tfDict   \n",
        "def forBinary(dict1,dict2,tempdoc):\n",
        "  tf1 = find_binary(dict1,dict2)\n",
        "  tf_doc = dict.fromkeys(tempdoc,0)\n",
        "  for word,c in tempdoc.items():\n",
        "    tf_doc[word]=find_binary(dict1,c)\n",
        "  return tf1,tf_doc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dtdXoT4xVKG"
      },
      "source": [
        "def find_rawCount(dict1,dict2):\n",
        "    tfDict = dict.fromkeys(dict1,0)\n",
        "    for word,count in dict1.items():\n",
        "      if word in dict2:\n",
        "        count=dict2[word]\n",
        "        tfDict[word] = count\n",
        "    return tfDict\n",
        "def forRawCount(dict1,dict2,tempdoc):\n",
        "  tf1 = find_rawCount(dict1,dict2)\n",
        "  tf_doc = dict.fromkeys(tempdoc,0)\n",
        "  for word,c in tempdoc.items():\n",
        "    tf_doc[word]=find_rawCount(dict1,c)\n",
        "  return tf1,tf_doc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbXbLh0gnnLd"
      },
      "source": [
        "def findTF(wordDict, dict1):\n",
        "    dict1Count = len(dict1)\n",
        "    tfDict = dict.fromkeys(dict1,0)\n",
        "    for word,count in dict1.items():\n",
        "      if word in wordDict:\n",
        "        count=wordDict[word]\n",
        "        tfDict[word] = count / float(dict1Count)\n",
        "    return tfDict\n",
        "def forTF(dict1,dict2,tempdoc):\n",
        "  tf1 = findTF(dict2, dict1)\n",
        "  tf_doc = dict.fromkeys(tempdoc,0)\n",
        "  for word,c in tempdoc.items():\n",
        "    tf_doc[word]=findTF(c,dict1)\n",
        "  return tf1,tf_doc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F82z5Ddm6z8w"
      },
      "source": [
        "def findlogTF(wordDict, dict1):\n",
        "    tfDict = dict.fromkeys(dict1,0)\n",
        "    for word,count in dict1.items():\n",
        "          tfDict[word] = 1 + np.log(count)\n",
        "    return tfDict\n",
        "def forLogTF(dict1,dict2,tempdoc):\n",
        "  tf1 = findlogTF(dict2, dict1)\n",
        "  tf_doc = dict.fromkeys(tempdoc,0)\n",
        "  for word,c in tempdoc.items():\n",
        "    tf_doc[word]=findlogTF(c,dict1)\n",
        "  return tf1,tf_doc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5yE74hH7GO-"
      },
      "source": [
        "def finddoublenormTF(wordDict, dict1):\n",
        "    max=0\n",
        "    for word,count in dict1.items():\n",
        "      if max<count:\n",
        "        max=count\n",
        "    tfDict = dict.fromkeys(dict1,0)\n",
        "    for word,count in dict1.items():\n",
        "        tfDict[word] = 0.5+((0.5*(count)/max))\n",
        "    return tfDict\n",
        "def forNorm(dict1,dict2,tempdoc):\n",
        "  tf1 = finddoublenormTF(dict2, dict1)\n",
        "  tf_doc = dict.fromkeys(tempdoc,0)\n",
        "  for word,c in tempdoc.items():\n",
        "    tf_doc[word]=finddoublenormTF(c,dict1)\n",
        "  return tf1,tf_doc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCH6K8iRod4T"
      },
      "source": [
        "# Compute IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQ1gn1jKogaj"
      },
      "source": [
        "def findIDF(doc,dict1,doc_frequency):\n",
        "  idfDict = dict.fromkeys(dict1,0)\n",
        "  for word,c in dict1.items():  \n",
        "    if word in doc:\n",
        "      if (word in doc_frequency):\n",
        "          count1 = doc_frequency[word]\n",
        "          # print(count1)\n",
        "      idfDict[word] = np.log(467 / (float(count1)+1))\n",
        "  return idfDict\n",
        "def forIDF(dict1,dict2,tempdoc,docFrequency):\n",
        "  idfs = findIDF(dict2,dict1,docFrequency)\n",
        "  idf_doc = dict.fromkeys(tempdoc,0)\n",
        "  for word,c in tempdoc.items():\n",
        "    idf_doc[word]=findIDF(c,dict1,docFrequency)\n",
        "  return idfs,idf_doc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_ZYTd-8px1F"
      },
      "source": [
        "# Compute TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F43JtY0spMEL"
      },
      "source": [
        "def findTFIDF(dicttf, idfs):\n",
        "  tfidf = {}\n",
        "  for word, i in dicttf.items():\n",
        "      tfidf[word] = i * idfs[word]\n",
        "  return tfidf\n",
        "def forTFIDF(tf1,idfs,tf_doc,idf_doc):\n",
        "  tfidf1 = findTFIDF(tf1, idfs)\n",
        "  df = pd.DataFrame([tfidf1])\n",
        "  tfidfdict = {}\n",
        "  for docid in range(0,467):\n",
        "    temp = {}\n",
        "    for word in tf_doc[0]:\n",
        "      temp[word] = tf_doc[docid][word]*idf_doc[docid][word]\n",
        "    tfidfdict[docid] = temp\n",
        "  return tfidf1,tfidfdict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25bDvL-kaL6q"
      },
      "source": [
        "# Compute Scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k64WsxLVbZG4"
      },
      "source": [
        "def getRank(doc,query):\n",
        "  score=dict.fromkeys(doc,0)\n",
        "  for i in range(467):\n",
        "    sum=0\n",
        "    for c in query:\n",
        "      sum+=doc[i][c]*query[c]\n",
        "    score[i]=sum \n",
        "  rankDoc = sorted(score, key=score.get, reverse=True)\n",
        "  return rankDoc,score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHP42ALfRoM_"
      },
      "source": [
        "#RAVI\n",
        "# document_list = [[1,2,3,4],[5,6,7,8]]\n",
        "def cosine_on_documents(documentsList,query):\n",
        "  cosine_similar_list = []\n",
        "  for i in documentsList:\n",
        "    l1 = i\n",
        "    l2 = query\n",
        "    c = 0\n",
        "    for i in range(len(l1)):\n",
        "            c+= l1[i]*l2[i]\n",
        "    cosine = c / float((sum(l1)*sum(l2))**0.5)\n",
        "    cosine_similar_list.append(cosine)\n",
        "  return cosine_similar_list\n",
        "\n",
        "# cosine_similar_list = cosine_on_documents(document_list,[1,2,3,4])\n",
        "# cosine_similar_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lfFjdySTDkA"
      },
      "source": [
        "def topDocuments(cosine_similar_list):\n",
        "  top_cosine_pos = []\n",
        "\n",
        "  max1 = 0\n",
        "  pos1 = 0\n",
        "\n",
        "  for i in range(5):\n",
        "    max1 = 0\n",
        "    pos1 = 0\n",
        "    for j in range(0,len(cosine_similar_list)):\n",
        "      flag = 0\n",
        "      for k in range(len(top_cosine_pos)):\n",
        "        if j==top_cosine_pos[k]:\n",
        "          flag = 1\n",
        "      if flag == 0:\n",
        "        temp = cosine_similar_list[j]\n",
        "        if(temp>max1):\n",
        "          max1 = temp\n",
        "          pos1 = j\n",
        "    max1\n",
        "    pos1\n",
        "    top_cosine_pos.append(pos1)\n",
        "  return top_cosine_pos\n",
        "\n",
        "# top_cosine_pos = topDocuments( cosine_similar_list  )\n",
        "# top_cosine_pos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5nL5vIDTSSt"
      },
      "source": [
        "def callingFun_cosine(choice,query,dict1,tempdoc,docFrequency,file1,file2,documentfilename):\n",
        "  query = cleaning(query,\" \",0)\n",
        "  query=query.split()\n",
        "  query = remove_stopword(0,query)\n",
        "  dict2 = query_count(query,dict1) \n",
        "  if choice == 'Binary':\n",
        "    tf1,tf_doc = forBinary(dict1,dict2,tempdoc)\n",
        "  elif choice == 'Raw count':\n",
        "    tf1,tf_doc = forRawCount(dict1,dict2,tempdoc)\n",
        "  elif choice == 'Term Frequency':\n",
        "    tf1,tf_doc = forTF(dict1,dict2,tempdoc)\n",
        "  elif choice == 'Log Normalization':\n",
        "    tf1,tf_doc = forLogTF(dict1,dict2,tempdoc)\n",
        "  elif choice == 'Double Normalization':\n",
        "    tf1,tf_doc = forNorm(dict1,dict2,tempdoc)\n",
        "  idfs,idf_doc = forIDF(dict1,dict2,tempdoc,docFrequency)\n",
        "  tfidf1,tfidfdict = forTFIDF(tf1,idfs,tf_doc,idf_doc)\n",
        "  list2doc=[]\n",
        "  for i,j in tfidfdict.items():\n",
        "    list1=[]\n",
        "    for k,m in j.items():\n",
        "      list1.append(m)\n",
        "    list2doc.append(list1)\n",
        "  list1query=[]\n",
        "  for k,m in tfidf1.items():\n",
        "       list1query.append(m)\n",
        "  #docranked , scores =getRank(list2doc,list1query)\n",
        "  list1=cosine_on_documents(list2doc,list1query)\n",
        "  docranked=topDocuments(list1)\n",
        "  #docranked=[]\n",
        "  scores=[]\n",
        "  return documentfilename, docranked , scores "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpNmRwABGcz8"
      },
      "source": [
        "dict1,tempdoc,docFrequency,file1,file2,documentfilename = loadStoredFiles()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNxGXer6iw4a"
      },
      "source": [
        "def callingFun(choice,query,dict1,tempdoc,docFrequency,file1,file2,documentfilename):\n",
        "  query = cleaning(query,\" \",0)\n",
        "  query=query.split()\n",
        "  query = remove_stopword(0,query)\n",
        "  dict2 = query_count(query,dict1) \n",
        "  if choice == 'Binary':\n",
        "    tf1,tf_doc = forBinary(dict1,dict2,tempdoc)\n",
        "  elif choice == 'Raw count':\n",
        "    tf1,tf_doc = forRawCount(dict1,dict2,tempdoc)\n",
        "  elif choice == 'Term Frequency':\n",
        "    tf1,tf_doc = forTF(dict1,dict2,tempdoc)\n",
        "  elif choice == 'Log Normalization':\n",
        "    tf1,tf_doc = forLogTF(dict1,dict2,tempdoc)\n",
        "  elif choice == 'Double Normalization':\n",
        "    tf1,tf_doc = forNorm(dict1,dict2,tempdoc)\n",
        "  idfs,idf_doc = forIDF(dict1,dict2,tempdoc,docFrequency)\n",
        "  tfidf1,tfidfdict = forTFIDF(tf1,idfs,tf_doc,idf_doc)\n",
        "  docranked , scores =getRank(tfidfdict,tfidf1)\n",
        "  return documentfilename, docranked , scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRUGGEGCTzMe",
        "outputId": "713c6eb2-d1e1-4657-c0c3-6a1c368e42f2"
      },
      "source": [
        "query = \"THE CRAB AND THE HERON\"\n",
        "choice='Double Normalization'\n",
        "print('------------------------------RESULTS WITH COSINE-------------------------------------')\n",
        "print('-------------------------------'+choice+'-------------------------------------')\n",
        "documentfilename,docranked , scores = callingFun_cosine(choice,query,dict1,tempdoc,docFrequency,file1,file2,documentfilename)\n",
        "for i in range(0,5):\n",
        "  print(\"Document Name \"+ \" : \"+ str(documentfilename[docranked[i]]))\n",
        "choice='Log Normalization'\n",
        "print('---------------------------------'+choice+'---------------------------------------')\n",
        "documentfilename,docranked , scores = callingFun_cosine(choice,query,dict1,tempdoc,docFrequency,file1,file2,documentfilename)\n",
        "for i in range(0,5):\n",
        "  print(\"Document Name \"+ \" : \"+ str(documentfilename[docranked[i]]))\n",
        "choice='Term Frequency'\n",
        "print('-------------------------------'+choice+'-------------------------------------')\n",
        "documentfilename,docranked , scores = callingFun_cosine(choice,query,dict1,tempdoc,docFrequency,file1,file2,documentfilename)\n",
        "for i in range(0,5):\n",
        "  print(\"Document Name \"+ \" : \"+ str(documentfilename[docranked[i]]))\n",
        "choice='Raw count'\n",
        "print('-----------------------------------'+choice+'-----------------------------------------')\n",
        "documentfilename,docranked , scores = callingFun_cosine(choice,query,dict1,tempdoc,docFrequency,file1,file2,documentfilename)\n",
        "for i in range(0,5):\n",
        "  print(\"Document Name \"+ \" : \"+ str(documentfilename[docranked[i]]))\n",
        "choice='Binary'\n",
        "print('-------------------------------------'+choice+'-----------------------------------------')\n",
        "documentfilename,docranked , scores = callingFun_cosine(choice,query,dict1,tempdoc,docFrequency,file1,file2,documentfilename)\n",
        "for i in range(0,5):\n",
        "  print(\"Document Name \"+ \" : \"+ str(documentfilename[docranked[i]]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------------------RESULTS WITH COSINE-------------------------------------\n",
            "-------------------------------Double Normalization-------------------------------------\n",
            "Document Name  :  'crabhern.txt'\n",
            "Document Name  :  'aesop11.txt'\n",
            "Document Name  :  'aesopa10.txt'\n",
            "Document Name  :  'long1-3.txt'\n",
            "Document Name  :  'fgoose.txt'\n",
            "---------------------------------Log Normalization---------------------------------------\n",
            "Document Name  :  'crabhern.txt'\n",
            "Document Name  :  'aesop11.txt'\n",
            "Document Name  :  'aesopa10.txt'\n",
            "Document Name  :  'long1-3.txt'\n",
            "Document Name  :  'fgoose.txt'\n",
            "-------------------------------Term Frequency-------------------------------------\n",
            "Document Name  :  'crabhern.txt'\n",
            "Document Name  :  'aesop11.txt'\n",
            "Document Name  :  'aesopa10.txt'\n",
            "Document Name  :  'long1-3.txt'\n",
            "Document Name  :  'timem.hac'\n",
            "-----------------------------------Raw count-----------------------------------------\n",
            "Document Name  :  'crabhern.txt'\n",
            "Document Name  :  'aesop11.txt'\n",
            "Document Name  :  'aesopa10.txt'\n",
            "Document Name  :  'long1-3.txt'\n",
            "Document Name  :  'timem.hac'\n",
            "-------------------------------------Binary-----------------------------------------\n",
            "Document Name  :  'crabhern.txt'\n",
            "Document Name  :  'aesop11.txt'\n",
            "Document Name  :  'aesopa10.txt'\n",
            "Document Name  :  'long1-3.txt'\n",
            "Document Name  :  'fgoose.txt'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mcQ2AzlaPWa"
      },
      "source": [
        "# Give Query and choice of Weighting Scheme and Get scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSyZvPEt5zil",
        "outputId": "967359da-7530-43b8-a4c7-04aedfa90e1b"
      },
      "source": [
        "query = \"THE CRAB AND THE HERON\"\n",
        "choice='Double Normalization'\n",
        "print('-------------------------RESULTS WITH TFIDF----------------------------------')\n",
        "print('-------------------------------'+choice+'-------------------------------------')\n",
        "documentfilename,docranked , scores = callingFun(choice,query,dict1,tempdoc,docFrequency,file1,file2,documentfilename)\n",
        "for i in range(0,5):\n",
        "  print(\"Document Name \"+ str(i+1) + \" : \"+ str(documentfilename[docranked[i]]) +\"    with TFIDF Scores :   \"+ str(scores[docranked[i]]) )\n",
        "choice='Log Normalization'\n",
        "print('---------------------------------'+choice+'---------------------------------------')\n",
        "documentfilename,docranked , scores = callingFun(choice,query,dict1,tempdoc,docFrequency,file1,file2,documentfilename)\n",
        "for i in range(0,5):\n",
        "  print(\"Document Name \"+ str(i+1) + \" : \"+ str(documentfilename[docranked[i]]) +\"    with TFIDF Scores :   \"+ str(scores[docranked[i]]) )\n",
        "choice='Term Frequency'\n",
        "print('-------------------------------'+choice+'-------------------------------------')\n",
        "documentfilename,docranked , scores = callingFun(choice,query,dict1,tempdoc,docFrequency,file1,file2,documentfilename)\n",
        "for i in range(0,5):\n",
        "  print(\"Document Name \"+ str(i+1) + \" : \"+ str(documentfilename[docranked[i]]) +\"    with TFIDF Scores :   \"+ str(scores[docranked[i]]) )\n",
        "choice='Raw count'\n",
        "print('-----------------------------------'+choice+'-----------------------------------------')\n",
        "documentfilename,docranked , scores = callingFun(choice,query,dict1,tempdoc,docFrequency,file1,file2,documentfilename)\n",
        "for i in range(0,5):\n",
        "  print(\"Document Name \"+ str(i+1) + \" : \"+ str(documentfilename[docranked[i]]) +\"    with TFIDF Scores :   \"+ str(scores[docranked[i]]) )\n",
        "choice='Binary'\n",
        "print('-------------------------------------'+choice+'-----------------------------------------')\n",
        "documentfilename,docranked , scores = callingFun(choice,query,dict1,tempdoc,docFrequency,file1,file2,documentfilename)\n",
        "for i in range(0,5):\n",
        "  print(\"Document Name \"+ str(i+1) + \" : \"+ str(documentfilename[docranked[i]]) +\"    with TFIDF Scores :   \"+ str(scores[docranked[i]]) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------------------RESULTS WITH TFIDF----------------------------------\n",
            "-------------------------------Double Normalization-------------------------------------\n",
            "Document Name 1 :  'aesop11.txt'    with TFIDF Scores :   10.814073147224516\n",
            "Document Name 2 :  'crabhern.txt'    with TFIDF Scores :   10.814073147224516\n",
            "Document Name 3 :  'aesopa10.txt'    with TFIDF Scores :   4.4321520400213394\n",
            "Document Name 4 :  'fgoose.txt'    with TFIDF Scores :   4.4321520400213394\n",
            "Document Name 5 :  'long1-3.txt'    with TFIDF Scores :   4.4321520400213394\n",
            "---------------------------------Log Normalization---------------------------------------\n",
            "Document Name 1 :  'aesop11.txt'    with TFIDF Scores :   625.3113742746723\n",
            "Document Name 2 :  'crabhern.txt'    with TFIDF Scores :   625.3113742746723\n",
            "Document Name 3 :  'aesopa10.txt'    with TFIDF Scores :   331.1335242513369\n",
            "Document Name 4 :  'fgoose.txt'    with TFIDF Scores :   331.1335242513369\n",
            "Document Name 5 :  'long1-3.txt'    with TFIDF Scores :   331.1335242513369\n",
            "-------------------------------Term Frequency-------------------------------------\n",
            "Document Name 1 :  'crabhern.txt'    with TFIDF Scores :   1.9867042456569383e-07\n",
            "Document Name 2 :  'aesop11.txt'    with TFIDF Scores :   5.813108762889941e-08\n",
            "Document Name 3 :  'timem.hac'    with TFIDF Scores :   3.123595524271839e-08\n",
            "Document Name 4 :  'aesopa10.txt'    with TFIDF Scores :   2.342696643203879e-08\n",
            "Document Name 5 :  'long1-3.txt'    with TFIDF Scores :   2.342696643203879e-08\n",
            "-----------------------------------Raw count-----------------------------------------\n",
            "Document Name 1 :  'crabhern.txt'    with TFIDF Scores :   448.8731935594525\n",
            "Document Name 2 :  'aesop11.txt'    with TFIDF Scores :   131.3405707271753\n",
            "Document Name 3 :  'timem.hac'    with TFIDF Scores :   70.57408275202454\n",
            "Document Name 4 :  'aesopa10.txt'    with TFIDF Scores :   52.93056206401841\n",
            "Document Name 5 :  'long1-3.txt'    with TFIDF Scores :   52.93056206401841\n",
            "-------------------------------------Binary-----------------------------------------\n",
            "Document Name 1 :  'aesop11.txt'    with TFIDF Scores :   43.12296728714463\n",
            "Document Name 2 :  'crabhern.txt'    with TFIDF Scores :   43.12296728714463\n",
            "Document Name 3 :  'aesopa10.txt'    with TFIDF Scores :   17.643520688006134\n",
            "Document Name 4 :  'fgoose.txt'    with TFIDF Scores :   17.643520688006134\n",
            "Document Name 5 :  'long1-3.txt'    with TFIDF Scores :   17.643520688006134\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kodk55uuL6F-"
      },
      "source": [
        "# top 10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ABOXuuEhxwF",
        "outputId": "9d0d0087-14c0-405a-d778-76c4c1eec2fa"
      },
      "source": [
        "for i in range(0,10):\n",
        "  print(\"Document Name \"+ str(i+1) + \" : \"+ str(documentfilename[docranked[i]]) +\"    with TFIDF Scores :   \"+ str(scores[docranked[i]]) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Document Name 1 :  'aesop11.txt'    with TFIDF Scores :   42.019022293601225\n",
            "Document Name 2 :  'crabhern.txt'    with TFIDF Scores :   42.019022293601225\n",
            "Document Name 3 :  'aesopa10.txt'    with TFIDF Scores :   16.539575694462723\n",
            "Document Name 4 :  'fgoose.txt'    with TFIDF Scores :   16.539575694462723\n",
            "Document Name 5 :  'long1-3.txt'    with TFIDF Scores :   16.539575694462723\n",
            "Document Name 6 :  'missing.txt'    with TFIDF Scores :   16.539575694462723\n",
            "Document Name 7 :  'timem.hac'    with TFIDF Scores :   16.539575694462723\n",
            "Document Name 8 : ['13chil.txt'    with TFIDF Scores :   0.0\n",
            "Document Name 9 :  'yukon.txt'    with TFIDF Scores :   0.0\n",
            "Document Name 10 :  '6napolen.txt'    with TFIDF Scores :   0.0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}